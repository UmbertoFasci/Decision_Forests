#+title: Decision Trees Test

The following document will contain the basic instructions for creating a decision tree model with tensorflow.
In this document I will:

1. Train a binary classification Random Forest on a dataset containing numerical, categorical, and missing data.
2. Evaluate the model on the test set.
3. Prepare the model for TensorFlow Serving
4. Examine the overall of the model and the importance of each feature.
5. Re-train the model with a different learning algorithm (Gradient Boost Decision Trees).
6. Use a different set of input features.
7. Change the hyperparameters of the model.
8. Preprocess the features.
9. Train the model for regression.

* Importing Libraries

#+begin_src jupyter-python
import tensorflow_decision_forests as tfdf

import os
import numpy as np
import pandas as pd
import tensorflow as tf
import math
#+end_src

#+RESULTS:

#+begin_src jupyter-python
print("Found TensorFlow Decision Forests v" + tfdf.__version__)
#+end_src

#+RESULTS:
: Found TensorFlow Decision Forests v1.3.0

* Training a Random Forest model

#+begin_src jupyter-python
# Download the dataset
!wget -q https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins.csv -O /tmp/penguins.csv

# Load the dataset into Pandas DataFrame
dataset_df = pd.read_csv("/tmp/penguins.csv")

# Display the first 3 examples
dataset_df.head(3)
#+end_src

#+RESULTS:
:   species     island  bill_length_mm  bill_depth_mm  flipper_length_mm
: 0  Adelie  Torgersen            39.1           18.7              181.0  \
: 1  Adelie  Torgersen            39.5           17.4              186.0
: 2  Adelie  Torgersen            40.3           18.0              195.0
:
:    body_mass_g     sex  year
: 0       3750.0    male  2007
: 1       3800.0  female  2007
: 2       3250.0  female  2007

#+begin_src jupyter-python
label = "species"

classes = dataset_df[label].unique().tolist()
print(f"Label classes: {classes}")

dataset_df[label] = dataset_df[label].map(classes.index)
#+end_src

#+RESULTS:
: Label classes: ['Adelie', 'Gentoo', 'Chinstrap']


#+begin_src jupyter-python
def split_dataset(dataset, test_ratio=0.30):
    test_indices = np.random.rand(len(dataset)) < test_ratio
    return dataset[~test_indices], dataset[test_indices]

train_ds_pd, test_ds_pd = split_dataset(dataset_df)
print("{} examples in training, {} examples for testing.".format(
    len(train_ds_pd), len(test_ds_pd)))
#+end_src

#+RESULTS:
: 235 examples in training, 109 examples for testing.

#+begin_src jupyter-python
train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)
test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label)
#+end_src

#+RESULTS:
: Metal device set to: Apple M1 Max

#+begin_src jupyter-python
# Specify the model
model_1 = tfdf.keras.RandomForestModel(verbose=2)

# Train the model
model_1.fit(train_ds)
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Use 10 thread(s) for training
Use /var/folders/41/4m81f87d31bdzxy69f3rzd1c0000gn/T/tmpwe5ry0gg as temporary training directory
Reading training dataset...
Training tensor examples:
Features: {'island': <tf.Tensor 'data:0' shape=(None,) dtype=string>, 'bill_length_mm': <tf.Tensor 'data_1:0' shape=(None,) dtype=float64>, 'bill_depth_mm': <tf.Tensor 'data_2:0' shape=(None,) dtype=float64>, 'flipper_length_mm': <tf.Tensor 'data_3:0' shape=(None,) dtype=float64>, 'body_mass_g': <tf.Tensor 'data_4:0' shape=(None,) dtype=float64>, 'sex': <tf.Tensor 'data_5:0' shape=(None,) dtype=string>, 'year': <tf.Tensor 'data_6:0' shape=(None,) dtype=int64>}
Label: Tensor("data_7:0", shape=(None,), dtype=int64)
Weights: None
Normalized tensor features:
 {'island': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data:0' shape=(None,) dtype=string>), 'bill_length_mm': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast:0' shape=(None,) dtype=float32>), 'bill_depth_mm': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_1:0' shape=(None,) dtype=float32>), 'flipper_length_mm': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_2:0' shape=(None,) dtype=float32>), 'body_mass_g': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_3:0' shape=(None,) dtype=float32>), 'sex': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_5:0' shape=(None,) dtype=string>), 'year': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_4:0' shape=(None,) dtype=float32>)}
2023-05-19 17:22:13.819025: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
Training dataset read in 0:00:02.049508. Found 235 examples.
Training model...
Standard output detected as not visible to the user e.g. running in a notebook. Creating a training log redirection. If training gets stuck, try calling tfdf.keras.set_training_logs_redirection(False).

systemMemory: 32.00 GB
maxCacheSize: 10.67 GB
[INFO 23-05-19 17:22:14.0487 CDT kernel.cc:773] Start Yggdrasil model training
[INFO 23-05-19 17:22:14.0498 CDT kernel.cc:774] Collect training examples
[INFO 23-05-19 17:22:14.0498 CDT kernel.cc:787] Dataspec guide:
column_guides {
  column_name_pattern: "^__LABEL$"
  type: CATEGORICAL
  categorial {
    min_vocab_frequency: 0
    max_vocab_count: -1
  }
}
default_column_guide {
  categorial {
    max_vocab_count: 2000
  }
  discretized_numerical {
    maximum_num_bins: 255
  }
}
ignore_columns_without_guides: false
detect_numerical_as_discretized_numerical: false
[INFO 23-05-19 17:22:14.0507 CDT kernel.cc:393] Number of batches: 1
[INFO 23-05-19 17:22:14.0507 CDT kernel.cc:394] Number of examples: 235
[INFO 23-05-19 17:22:14.0509 CDT kernel.cc:794] Training dataset:
Number of records: 235
Number of columns: 8

Number of columns by type:
	NUMERICAL: 5 (62.5%)
	CATEGORICAL: 3 (37.5%)

Columns:

NUMERICAL: 5 (62.5%)
	1: "bill_depth_mm" NUMERICAL num-nas:2 (0.851064%) mean:17.1133 min:13.1 max:21.5 sd:1.96566
	2: "bill_length_mm" NUMERICAL num-nas:2 (0.851064%) mean:44.0124 min:32.1 max:59.6 sd:5.50772
	3: "body_mass_g" NUMERICAL num-nas:2 (0.851064%) mean:4178.86 min:2700 max:6050 sd:796.897
	4: "flipper_length_mm" NUMERICAL num-nas:2 (0.851064%) mean:200.768 min:172 max:231 sd:13.9379
	7: "year" NUMERICAL mean:2008.04 min:2007 max:2009 sd:0.818859

CATEGORICAL: 3 (37.5%)
	0: "__LABEL" CATEGORICAL integerized vocab-size:4 no-ood-item
	5: "island" CATEGORICAL has-dict vocab-size:4 zero-ood-items most-frequent:"Biscoe" 107 (45.5319%)
	6: "sex" CATEGORICAL num-nas:10 (4.25532%) has-dict vocab-size:3 zero-ood-items most-frequent:"female" 115 (51.1111%)

Terminology:
	nas: Number of non-available (i.e. missing) values.
	ood: Out of dictionary.
	manually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.
	tokenized: The attribute value is obtained through tokenization.
	has-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.
	vocab-size: Number of unique values.

[INFO 23-05-19 17:22:14.0512 CDT kernel.cc:810] Configure learner
[INFO 23-05-19 17:22:14.0513 CDT kernel.cc:824] Training config:
learner: "RANDOM_FOREST"
features: "^bill_depth_mm$"
features: "^bill_length_mm$"
features: "^body_mass_g$"
features: "^flipper_length_mm$"
features: "^island$"
features: "^sex$"
features: "^year$"
label: "^__LABEL$"
task: CLASSIFICATION
random_seed: 123456
metadata {
  framework: "TF Keras"
}
pure_serving_model: false
[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {
  num_trees: 300
  decision_tree {
    max_depth: 16
    min_examples: 5
    in_split_min_examples_check: true
    keep_non_leaf_label_distribution: true
    num_candidate_attributes: 0
    missing_value_policy: GLOBAL_IMPUTATION
    allow_na_conditions: false
    categorical_set_greedy_forward {
      sampling: 0.1
      max_num_items: -1
      min_item_frequency: 1
    }
    growing_strategy_local {
    }
    categorical {
      cart {
      }
    }
    axis_aligned_split {
    }
    internal {
      sorting_strategy: PRESORTED
    }
    uplift {
      min_examples_in_treatment: 5
      split_score: KULLBACK_LEIBLER
    }
  }
  winner_take_all_inference: true
  compute_oob_performances: true
  compute_oob_variable_importances: false
  num_oob_variable_importances_permutations: 1
  bootstrap_training_dataset: true
  bootstrap_size_ratio: 1
  adapt_bootstrap_size_ratio_for_maximum_training_duration: false
  sampling_with_replacement: true
}

[INFO 23-05-19 17:22:14.0514 CDT kernel.cc:827] Deployment config:
cache_path: "/var/folders/41/4m81f87d31bdzxy69f3rzd1c0000gn/T/tmpwe5ry0gg/working_cache"
num_threads: 10
try_resume_training: true
[INFO 23-05-19 17:22:14.0518 CDT kernel.cc:889] Train model
[INFO 23-05-19 17:22:14.0520 CDT random_forest.cc:416] Training random forest on 235 example(s) and 7 feature(s).
[INFO 23-05-19 17:22:14.0539 CDT random_forest.cc:805] Training of tree  1/300 (tree index:8) done accuracy:0.910112 logloss:3.23988
[INFO 23-05-19 17:22:14.0541 CDT random_forest.cc:805] Training of tree  12/300 (tree index:12) done accuracy:0.943231 logloss:1.30503
[INFO 23-05-19 17:22:14.0543 CDT random_forest.cc:805] Training of tree  23/300 (tree index:22) done accuracy:0.953192 logloss:0.394233
[INFO 23-05-19 17:22:14.0545 CDT random_forest.cc:805] Training of tree  35/300 (tree index:35) done accuracy:0.965957 logloss:0.390056
[INFO 23-05-19 17:22:14.0547 CDT random_forest.cc:805] Training of tree  45/300 (tree index:45) done accuracy:0.970213 logloss:0.241466
[INFO 23-05-19 17:22:14.0550 CDT random_forest.cc:805] Training of tree  56/300 (tree index:58) done accuracy:0.970213 logloss:0.240155
[INFO 23-05-19 17:22:14.0551 CDT random_forest.cc:805] Training of tree  68/300 (tree index:67) done accuracy:0.970213 logloss:0.240307
[INFO 23-05-19 17:22:14.0554 CDT random_forest.cc:805] Training of tree  80/300 (tree index:75) done accuracy:0.970213 logloss:0.240302
[INFO 23-05-19 17:22:14.0556 CDT random_forest.cc:805] Training of tree  90/300 (tree index:89) done accuracy:0.970213 logloss:0.0997129
[INFO 23-05-19 17:22:14.0557 CDT random_forest.cc:805] Training of tree  100/300 (tree index:100) done accuracy:0.978723 logloss:0.0949417
[INFO 23-05-19 17:22:14.0559 CDT random_forest.cc:805] Training of tree  110/300 (tree index:109) done accuracy:0.974468 logloss:0.0953088
[INFO 23-05-19 17:22:14.0561 CDT random_forest.cc:805] Training of tree  122/300 (tree index:120) done accuracy:0.974468 logloss:0.096616
[INFO 23-05-19 17:22:14.0563 CDT random_forest.cc:805] Training of tree  132/300 (tree index:130) done accuracy:0.974468 logloss:0.0966673
[INFO 23-05-19 17:22:14.0566 CDT random_forest.cc:805] Training of tree  142/300 (tree index:141) done accuracy:0.974468 logloss:0.0968194
[INFO 23-05-19 17:22:14.0567 CDT random_forest.cc:805] Training of tree  152/300 (tree index:152) done accuracy:0.974468 logloss:0.0967422
[INFO 23-05-19 17:22:14.0570 CDT random_forest.cc:805] Training of tree  163/300 (tree index:162) done accuracy:0.974468 logloss:0.0952728
[INFO 23-05-19 17:22:14.0572 CDT random_forest.cc:805] Training of tree  175/300 (tree index:175) done accuracy:0.970213 logloss:0.0952571
[INFO 23-05-19 17:22:14.0573 CDT random_forest.cc:805] Training of tree  185/300 (tree index:174) done accuracy:0.970213 logloss:0.095572
[INFO 23-05-19 17:22:14.0576 CDT random_forest.cc:805] Training of tree  195/300 (tree index:195) done accuracy:0.970213 logloss:0.0959348
[INFO 23-05-19 17:22:14.0577 CDT random_forest.cc:805] Training of tree  206/300 (tree index:200) done accuracy:0.970213 logloss:0.0964578
[INFO 23-05-19 17:22:14.0579 CDT random_forest.cc:805] Training of tree  216/300 (tree index:215) done accuracy:0.974468 logloss:0.0959382
[INFO 23-05-19 17:22:14.0581 CDT random_forest.cc:805] Training of tree  226/300 (tree index:225) done accuracy:0.978723 logloss:0.096229
[INFO 23-05-19 17:22:14.0583 CDT random_forest.cc:805] Training of tree  237/300 (tree index:236) done accuracy:0.974468 logloss:0.0973237
[INFO 23-05-19 17:22:14.0585 CDT random_forest.cc:805] Training of tree  249/300 (tree index:248) done accuracy:0.974468 logloss:0.0985853
[INFO 23-05-19 17:22:14.0588 CDT random_forest.cc:805] Training of tree  260/300 (tree index:259) done accuracy:0.974468 logloss:0.0982098
[INFO 23-05-19 17:22:14.0590 CDT random_forest.cc:805] Training of tree  271/300 (tree index:270) done accuracy:0.974468 logloss:0.0980791
[INFO 23-05-19 17:22:14.0591 CDT random_forest.cc:805] Training of tree  281/300 (tree index:280) done accuracy:0.974468 logloss:0.0984111
[INFO 23-05-19 17:22:14.0593 CDT random_forest.cc:805] Training of tree  291/300 (tree index:291) done accuracy:0.974468 logloss:0.0987725
[INFO 23-05-19 17:22:14.0595 CDT random_forest.cc:805] Training of tree  300/300 (tree index:295) done accuracy:0.974468 logloss:0.0994343
[INFO 23-05-19 17:22:14.0595 CDT random_forest.cc:885] Final OOB metrics: accuracy:0.974468 logloss:0.0994343
[INFO 23-05-19 17:22:14.0598 CDT kernel.cc:926] Export model in log directory: /var/folders/41/4m81f87d31bdzxy69f3rzd1c0000gn/T/tmpwe5ry0gg with prefix d2471fb3ff7c4a40
[INFO 23-05-19 17:22:14.0628 CDT kernel.cc:944] Save model in resources
[INFO 23-05-19 17:22:14.0653 CDT abstract_model.cc:849] Model self evaluation:
Number of predictions (without weights): 235
Number of predictions (with weights): 235
Task: CLASSIFICATION
Label: __LABEL

Accuracy: 0.974468  CI95[W][0.950232 0.988824]
LogLoss: : 0.0994343
ErrorRate: : 0.0255319

Default Accuracy: : 0.434043
Default LogLoss: : 1.05911
Default ErrorRate: : 0.565957

Confusion Table:
truth\prediction
   0   1   2   3
0  0   0   0   0
1  0  99   1   2
2  0   1  82   0
3  0   2   0  48
Total: 235

One vs other classes:
[INFO 23-05-19 17:22:14.0723 CDT kernel.cc:1242] Loading model from path /var/folders/41/4m81f87d31bdzxy69f3rzd1c0000gn/T/tmpwe5ry0gg/model/ with prefix d2471fb3ff7c4a40
[INFO 23-05-19 17:22:14.0805 CDT decision_forest.cc:660] Model loaded with 300 root(s), 4634 node(s), and 7 input feature(s).
[INFO 23-05-19 17:22:14.0805 CDT abstract_model.cc:1312] Engine "RandomForestGeneric" built
[INFO 23-05-19 17:22:14.0805 CDT kernel.cc:1074] Use fast generic engine
Model trained in 0:00:00.036589
Compiling model...
WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x2b00f4d30> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x2b00f4d30> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x2b00f4d30> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Model compiled.
#+end_example
: <keras.callbacks.History at 0x2b00d5bd0>
:END:
* Evaluate the model

#+begin_src jupyter-python
model_1.compile(metrics=["accuracy"])
evaluation = model_1.evaluate(test_ds, return_dict=True)
print()

for name, value in evaluation.items():
    print(f"{name}: {value:.4f}")
#+end_src

#+RESULTS:
:RESULTS:
: 1/1 [==============================] - 0s 454ms/step - loss: 0.0000e+00 - accuracy: 1.0000
:
:
: loss: 0.0000
: accuracy: 1.0000
:END:

* TensorFlow Serving

#+begin_src jupyter-python
model_1.save("/tmp/my_saved_model")
#+end_src

#+RESULTS:
: WARNING:absl:Found untraced functions such as call_get_leaves while saving (showing 1 of 1). These functions will not be directly callable after loading.
: INFO:tensorflow:Assets written to: /tmp/my_saved_model/assets
: INFO:tensorflow:Assets written to: /tmp/my_saved_model/assets

* Model structure and feature importance

#+begin_src jupyter-python
model_1.summary()
#+end_src

#+RESULTS:
#+begin_example
Model: "random_forest_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
=================================================================
Total params: 1
Trainable params: 0
Non-trainable params: 1
_________________________________________________________________
Type: "RANDOM_FOREST"
Task: CLASSIFICATION
Label: "__LABEL"

Input Features (7):
	bill_depth_mm
	bill_length_mm
	body_mass_g
	flipper_length_mm
	island
	sex
	year

No weights

Variable Importance: INV_MEAN_MIN_DEPTH:
    1. "flipper_length_mm"  0.426447 ################
    2.    "bill_length_mm"  0.409912 ##############
    3.            "island"  0.327954 #######
    4.     "bill_depth_mm"  0.302838 #####
    5.       "body_mass_g"  0.265318 ##
    6.               "sex"  0.233728
    7.              "year"  0.233178

Variable Importance: NUM_AS_ROOT:
    1. "flipper_length_mm" 153.000000 ################
    2.    "bill_length_mm" 55.000000 #####
    3.            "island" 45.000000 ####
    4.     "bill_depth_mm" 42.000000 ####
    5.       "body_mass_g"  5.000000

Variable Importance: NUM_NODES:
    1.    "bill_length_mm" 678.000000 ################
    2.     "bill_depth_mm" 442.000000 ##########
    3. "flipper_length_mm" 374.000000 ########
    4.            "island" 314.000000 #######
    5.       "body_mass_g" 305.000000 ######
    6.               "sex" 29.000000
    7.              "year" 25.000000

Variable Importance: SUM_SCORE:
    1.    "bill_length_mm" 23086.665045 ################
    2. "flipper_length_mm" 22279.785376 ###############
    3.            "island" 12917.421690 ########
    4.     "bill_depth_mm" 9287.342407 ######
    5.       "body_mass_g" 3417.259439 ##
    6.               "sex" 207.275119
    7.              "year" 70.325964



Winner takes all: true
Out-of-bag evaluation: accuracy:0.974468 logloss:0.0994343
Number of trees: 300
Total number of nodes: 4634

Number of nodes by tree:
Count: 300 Average: 15.4467 StdDev: 3.04639
Min: 9 Max: 27 Ignored: 0
----------------------------------------------
[  9, 10)  3   1.00%   1.00%
[ 10, 11)  0   0.00%   1.00%
[ 11, 12) 27   9.00%  10.00% ###
[ 12, 13)  0   0.00%  10.00%
[ 13, 14) 68  22.67%  32.67% #######
[ 14, 15)  0   0.00%  32.67%
[ 15, 16) 94  31.33%  64.00% ##########
[ 16, 17)  0   0.00%  64.00%
[ 17, 18) 59  19.67%  83.67% ######
[ 18, 19)  0   0.00%  83.67%
[ 19, 20) 25   8.33%  92.00% ###
[ 20, 21)  0   0.00%  92.00%
[ 21, 22) 12   4.00%  96.00% #
[ 22, 23)  0   0.00%  96.00%
[ 23, 24)  9   3.00%  99.00% #
[ 24, 25)  0   0.00%  99.00%
[ 25, 26)  1   0.33%  99.33%
[ 26, 27)  0   0.00%  99.33%
[ 27, 27]  2   0.67% 100.00%

Depth by leafs:
Count: 2467 Average: 3.38265 StdDev: 1.04359
Min: 1 Max: 7 Ignored: 0
----------------------------------------------
[ 1, 2)   8   0.32%   0.32%
[ 2, 3) 560  22.70%  23.02% #######
[ 3, 4) 786  31.86%  54.88% ##########
[ 4, 5) 761  30.85%  85.73% ##########
[ 5, 6) 303  12.28%  98.01% ####
[ 6, 7)  43   1.74%  99.76% #
[ 7, 7]   6   0.24% 100.00%

Number of training obs by leaf:
Count: 2467 Average: 28.5772 StdDev: 29.2001
Min: 5 Max: 107 Ignored: 0
----------------------------------------------
[   5,  10) 1242  50.34%  50.34% ##########
[  10,  15)  106   4.30%  54.64% #
[  15,  20)   83   3.36%  58.01% #
[  20,  25)   71   2.88%  60.88% #
[  25,  30)   64   2.59%  63.48% #
[  30,  35)   64   2.59%  66.07% #
[  35,  41)  111   4.50%  70.57% #
[  41,  46)   83   3.36%  73.94% #
[  46,  51)   66   2.68%  76.61% #
[  51,  56)   36   1.46%  78.07%
[  56,  61)   40   1.62%  79.69%
[  61,  66)   38   1.54%  81.23%
[  66,  71)   53   2.15%  83.38%
[  71,  77)   96   3.89%  87.27% #
[  77,  82)  129   5.23%  92.50% #
[  82,  87)   80   3.24%  95.74% #
[  87,  92)   57   2.31%  98.05%
[  92,  97)   34   1.38%  99.43%
[  97, 102)   10   0.41%  99.84%
[ 102, 107]    4   0.16% 100.00%

Attribute in nodes:
	678 : bill_length_mm [NUMERICAL]
	442 : bill_depth_mm [NUMERICAL]
	374 : flipper_length_mm [NUMERICAL]
	314 : island [CATEGORICAL]
	305 : body_mass_g [NUMERICAL]
	29 : sex [CATEGORICAL]
	25 : year [NUMERICAL]

Attribute in nodes with depth <= 0:
	153 : flipper_length_mm [NUMERICAL]
	55 : bill_length_mm [NUMERICAL]
	45 : island [CATEGORICAL]
	42 : bill_depth_mm [NUMERICAL]
	5 : body_mass_g [NUMERICAL]

Attribute in nodes with depth <= 1:
	234 : flipper_length_mm [NUMERICAL]
	231 : bill_length_mm [NUMERICAL]
	191 : island [CATEGORICAL]
	164 : bill_depth_mm [NUMERICAL]
	71 : body_mass_g [NUMERICAL]
	1 : year [NUMERICAL]

Attribute in nodes with depth <= 2:
	457 : bill_length_mm [NUMERICAL]
	307 : bill_depth_mm [NUMERICAL]
	295 : flipper_length_mm [NUMERICAL]
	276 : island [CATEGORICAL]
	169 : body_mass_g [NUMERICAL]
	8 : sex [CATEGORICAL]
	4 : year [NUMERICAL]

Attribute in nodes with depth <= 3:
	610 : bill_length_mm [NUMERICAL]
	403 : bill_depth_mm [NUMERICAL]
	348 : flipper_length_mm [NUMERICAL]
	307 : island [CATEGORICAL]
	274 : body_mass_g [NUMERICAL]
	26 : sex [CATEGORICAL]
	10 : year [NUMERICAL]

Attribute in nodes with depth <= 5:
	678 : bill_length_mm [NUMERICAL]
	441 : bill_depth_mm [NUMERICAL]
	373 : flipper_length_mm [NUMERICAL]
	314 : island [CATEGORICAL]
	305 : body_mass_g [NUMERICAL]
	29 : sex [CATEGORICAL]
	24 : year [NUMERICAL]

Condition type in nodes:
	1824 : HigherCondition
	343 : ContainsBitmapCondition
Condition type in nodes with depth <= 0:
	255 : HigherCondition
	45 : ContainsBitmapCondition
Condition type in nodes with depth <= 1:
	701 : HigherCondition
	191 : ContainsBitmapCondition
Condition type in nodes with depth <= 2:
	1232 : HigherCondition
	284 : ContainsBitmapCondition
Condition type in nodes with depth <= 3:
	1645 : HigherCondition
	333 : ContainsBitmapCondition
Condition type in nodes with depth <= 5:
	1821 : HigherCondition
	343 : ContainsBitmapCondition
Node format: NOT_SET

Training OOB:
	trees: 1, Out-of-bag evaluation: accuracy:0.910112 logloss:3.23988
	trees: 12, Out-of-bag evaluation: accuracy:0.943231 logloss:1.30503
	trees: 23, Out-of-bag evaluation: accuracy:0.953192 logloss:0.394233
	trees: 35, Out-of-bag evaluation: accuracy:0.965957 logloss:0.390056
	trees: 45, Out-of-bag evaluation: accuracy:0.970213 logloss:0.241466
	trees: 56, Out-of-bag evaluation: accuracy:0.970213 logloss:0.240155
	trees: 68, Out-of-bag evaluation: accuracy:0.970213 logloss:0.240307
	trees: 80, Out-of-bag evaluation: accuracy:0.970213 logloss:0.240302
	trees: 90, Out-of-bag evaluation: accuracy:0.970213 logloss:0.0997129
	trees: 100, Out-of-bag evaluation: accuracy:0.978723 logloss:0.0949417
	trees: 110, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0953088
	trees: 122, Out-of-bag evaluation: accuracy:0.974468 logloss:0.096616
	trees: 132, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0966673
	trees: 142, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0968194
	trees: 152, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0967422
	trees: 163, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0952728
	trees: 175, Out-of-bag evaluation: accuracy:0.970213 logloss:0.0952571
	trees: 185, Out-of-bag evaluation: accuracy:0.970213 logloss:0.095572
	trees: 195, Out-of-bag evaluation: accuracy:0.970213 logloss:0.0959348
	trees: 206, Out-of-bag evaluation: accuracy:0.970213 logloss:0.0964578
	trees: 216, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0959382
	trees: 226, Out-of-bag evaluation: accuracy:0.978723 logloss:0.096229
	trees: 237, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0973237
	trees: 249, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0985853
	trees: 260, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0982098
	trees: 271, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0980791
	trees: 281, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0984111
	trees: 291, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0987725
	trees: 300, Out-of-bag evaluation: accuracy:0.974468 logloss:0.0994343
#+end_example

* Using make_inspector

#+begin_src jupyter-python
model_1.make_inspector().features()
#+end_src

#+RESULTS:
: '("bill_depth_mm" (1; #1)
:  "bill_length_mm" (1; #2)
:  "body_mass_g" (1; #3)
:  "flipper_length_mm" (1; #4)
:  "island" (4; #5)
:  "sex" (4; #6)
:  "year" (1; #7))

#+begin_src jupyter-python
model_1.make_inspector().variable_importances()
#+end_src

#+RESULTS:
#+begin_example
'("NUM_AS_ROOT": (("flipper_length_mm" (1; #4)  153.0)
  ("bill_length_mm" (1; #2)  55.0)
  ("island" (4; #5)  45.0)
  ("bill_depth_mm" (1; #1)  42.0)
  ("body_mass_g" (1; #3)  5.0))
 "SUM_SCORE": (("bill_length_mm" (1; #2)  23086.665044510737)
  ("flipper_length_mm" (1; #4)  22279.785376295447)
  ("island" (4; #5)  12917.421690158546)
  ("bill_depth_mm" (1; #1)  9287.342406939715)
  ("body_mass_g" (1; #3)  3417.259438963607)
  ("sex" (4; #6)  207.27511917054653)
  ("year" (1; #7)  70.3259641379118))
 "INV_MEAN_MIN_DEPTH": (("flipper_length_mm" (1; #4)  0.42644681553242786)
  ("bill_length_mm" (1; #2)  0.40991152011228976)
  ("island" (4; #5)  0.32795446212687934)
  ("bill_depth_mm" (1; #1)  0.3028375303457658)
  ("body_mass_g" (1; #3)  0.2653179954885292)
  ("sex" (4; #6)  0.233727719348659)
  ("year" (1; #7)  0.23317794025648098))
 "NUM_NODES": (("bill_length_mm" (1; #2)  678.0)
  ("bill_depth_mm" (1; #1)  442.0)
  ("flipper_length_mm" (1; #4)  374.0)
  ("island" (4; #5)  314.0)
  ("body_mass_g" (1; #3)  305.0)
  ("sex" (4; #6)  29.0)
  ("year" (1; #7)  25.0)))
#+end_example

* Model self evaluation

#+begin_src jupyter-python
model_1.make_inspector().evaluation()
#+end_src

#+RESULTS:
: Evaluation(num_examples=235, accuracy=0.9744680851063829, loss=0.09943434574264795, rmse=None, ndcg=None, aucs=None, auuc=None, qini=None)

* Plotting the training logs

#+begin_src jupyter-python
model_1.make_inspector().training_logs()
#+end_src

#+RESULTS:
| TrainLog | (num_trees=1 evaluation=Evaluation (num_examples=89 accuracy=0.9101123595505618 loss=3.2398787937807234 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=12 evaluation=Evaluation (num_examples=229 accuracy=0.9432314410480349 loss=1.3050296110747683 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=23 evaluation=Evaluation (num_examples=235 accuracy=0.9531914893617022 loss=0.39423320943370777 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=35 evaluation=Evaluation (num_examples=235 accuracy=0.9659574468085106 loss=0.3900564617616065 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=45 evaluation=Evaluation (num_examples=235 accuracy=0.9702127659574468 loss=0.24146640841314132 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=56 evaluation=Evaluation (num_examples=235 accuracy=0.9702127659574468 loss=0.24015527456364733 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=68 evaluation=Evaluation (num_examples=235 accuracy=0.9702127659574468 loss=0.24030704718637974 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=80 evaluation=Evaluation (num_examples=235 accuracy=0.9702127659574468 loss=0.24030178147269057 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=90 evaluation=Evaluation (num_examples=235 accuracy=0.9702127659574468 loss=0.09971290459024146 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=100 evaluation=Evaluation (num_examples=235 accuracy=0.9787234042553191 loss=0.09494173875514497 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=110 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.0953087846015362 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=122 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.09661599706779135 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=132 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.09666731662731222 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=142 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.09681942925808278 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=152 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.09674216249680266 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=163 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.095272810959277 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=175 evaluation=Evaluation (num_examples=235 accuracy=0.9702127659574468 loss=0.0952570849672911 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=185 evaluation=Evaluation (num_examples=235 accuracy=0.9702127659574468 loss=0.09557198714068596 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=195 evaluation=Evaluation (num_examples=235 accuracy=0.9702127659574468 loss=0.09593477687065272 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=206 evaluation=Evaluation (num_examples=235 accuracy=0.9702127659574468 loss=0.09645782659186962 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=216 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.09593824128362727 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=226 evaluation=Evaluation (num_examples=235 accuracy=0.9787234042553191 loss=0.09622902777045965 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=237 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.0973236724615414 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=249 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.0985852888488072 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=260 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.09820979333859174 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=271 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.09807908718890332 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=281 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.0984111214056611 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=291 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.09877253035281566 rmse=None ndcg=None aucs=None auuc=None qini=None)) | TrainLog | (num_trees=300 evaluation=Evaluation (num_examples=235 accuracy=0.9744680851063829 loss=0.09943434574264795 rmse=None ndcg=None aucs=None auuc=None qini=None)) |

#+begin_src jupyter-python
import matplotlib.pyplot as plt

logs = model_1.make_inspector().training_logs()

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])
plt.xlabel("Number of trees")
plt.ylabel("Accuracy (out-of-bag)")

plt.subplot(1, 2, 2)
plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])
plt.xlabel("Number of trees")
plt.ylabel("Logloss (out-of-bag)")

plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/85c2eae1ddaca27ae61ca502e5a0d500a1387342.png]]


* Retrain model with different learning algorithm


#+begin_src jupyter-python
tfdf.keras.get_all_models()
#+end_src

#+RESULTS:
| tensorflow_decision_forests.keras.RandomForestModel | tensorflow_decision_forests.keras.GradientBoostedTreesModel | tensorflow_decision_forests.keras.CartModel | tensorflow_decision_forests.keras.DistributedGradientBoostedTreesModel |


* Using a subset of features

#+begin_src jupyter-python
feature_1 = tfdf.keras.FeatureUsage(name="bill_length_mm")
feature_2 = tfdf.keras.FeatureUsage(name="island")

all_features = [feature_1, feature_2]

# This model is only being trained on two features.
# It will NOT be as good as the previous model trained on all features.

model_2 = tfdf.keras.GradientBoostedTreesModel(
    features=all_features, exclude_non_specified_features=True)

model_2.compile(metrics=["accuracy"])
model_2.fit(train_ds, validation_data=test_ds)

print(model_2.evaluate(test_ds, return_dict=True))
#+end_src

#+RESULTS:
#+begin_example
Use /var/folders/41/4m81f87d31bdzxy69f3rzd1c0000gn/T/tmp1gfgsesw as temporary training directory
Reading training dataset...
Training dataset read in 0:00:00.066490. Found 235 examples.
Reading validation dataset...
Num validation examples: tf.Tensor(109, shape=(), dtype=int32)
Validation dataset read in 0:00:00.090159. Found 109 examples.
Training model...
[WARNING 23-05-19 17:23:58.0331 CDT gradient_boosted_trees.cc:1797] "goss_alpha" set but "sampling_method" not equal to "GOSS".
[WARNING 23-05-19 17:23:58.0331 CDT gradient_boosted_trees.cc:1808] "goss_beta" set but "sampling_method" not equal to "GOSS".
[WARNING 23-05-19 17:23:58.0331 CDT gradient_boosted_trees.cc:1822] "selective_gradient_boosting_ratio" set but "sampling_method" not equal to "SELGB".
Model trained in 0:00:00.097217
Compiling model...
Model compiled.
1/1 [==============================] - 0s 48ms/step - loss: 0.0000e+00 - accuracy: 0.9817
{'loss': 0.0, 'accuracy': 0.9816513657569885}
[INFO 23-05-19 17:23:58.2883 CDT kernel.cc:1242] Loading model from path /var/folders/41/4m81f87d31bdzxy69f3rzd1c0000gn/T/tmp1gfgsesw/model/ with prefix dabcb66459274b14
[INFO 23-05-19 17:23:58.2954 CDT decision_forest.cc:660] Model loaded with 168 root(s), 5282 node(s), and 2 input feature(s).
[INFO 23-05-19 17:23:58.2954 CDT abstract_model.cc:1312] Engine "GradientBoostedTreesGeneric" built
[INFO 23-05-19 17:23:58.2954 CDT kernel.cc:1074] Use fast generic engine
#+end_example
